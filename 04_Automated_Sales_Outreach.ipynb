{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc674f48",
   "metadata": {},
   "source": [
    "- A workflow of Agent calls\n",
    "- An Agent that can use a Tool\n",
    "- An agent that can all on other agents (Tools or Handoffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207cc9e2",
   "metadata": {},
   "source": [
    "### Setup the email sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c9f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install brevo-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f37d4328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe8cf869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brevo_python\n",
    "from brevo_python.api.transactional_emails_api import TransactionalEmailsApi\n",
    "from brevo_python.models.send_smtp_email import SendSmtpEmail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4466c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configure API Key\n",
    "configuration = brevo_python.Configuration()\n",
    "configuration.api_key[\"api-key\"] = os.getenv(\"BREVO_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "096b3aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create API Client\n",
    "api_client = brevo_python.ApiClient(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4f64f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transactional Email API Instance\n",
    "email_api = TransactionalEmailsApi(api_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9b1ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Email Content\n",
    "email = SendSmtpEmail(\n",
    "    sender={\"name\": \"Ayushi Saxena\", \"email\": \"khonthedark@gmail.com\"},\n",
    "    to=[{\"email\": \"windwatcher17@gmail.com\", \"name\": \"Receiver\"}],\n",
    "    subject=\"Hello from Brevo Python SDK\",\n",
    "    html_content=\"<h2>Brevo Email Sent Successfully ðŸš€</h2>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d0b8241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Email sent successfully!\n",
      "{'message_id': '<202601310658.59517322209@smtp-relay.mailin.fr>',\n",
      " 'message_ids': None}\n"
     ]
    }
   ],
   "source": [
    "# Send Email\n",
    "try:\n",
    "    response = email_api.send_transac_email(email)\n",
    "    print(\"âœ… Email sent successfully!\")\n",
    "    print(response)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"âŒ Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da622693",
   "metadata": {},
   "source": [
    "## Agent Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82c26652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner, trace, function_tool\n",
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "from typing import Dict\n",
    "import os\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8db7fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system prompts\n",
    "instructions1 = \"You are a sales agent working for ComplAI, \\\n",
    "a company that provides a SaaS tool for ensuring SOC2 compliance and preparing for audits, powered by AI. \\\n",
    "You write professional, serious cold emails.\"\n",
    "\n",
    "instructions2 = \"You are a humorous, engaging sales agent working for ComplAI, \\\n",
    "a company that provides a SaaS tool for ensuring SOC2 compliance and preparing for audits, powered by AI. \\\n",
    "You write witty, engaging cold emails that are likely to get a response.\"\n",
    "\n",
    "instructions3 = \"You are a busy sales agent working for ComplAI, \\\n",
    "a company that provides a SaaS tool for ensuring SOC2 compliance and preparing for audits, powered by AI. \\\n",
    "You write concise, to the point cold emails.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7647e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three agents\n",
    "\n",
    "sales_agent1 = Agent(name=\"Professional Sales Agent\", instructions=instructions1, model=\"gpt-4o-mini\")\n",
    "sales_agent2 = Agent(name=\"Engaging Sales Agent\", instructions=instructions2, model=\"gpt-4o-mini\")\n",
    "sales_agent3 = Agent(name=\"Busy Sales Agent\", instructions=instructions3, model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82dd0655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error streaming response: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAPIError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run an agent\u001b[39;00m\n\u001b[32m      3\u001b[39m result = Runner.run_streamed(sales_agent1, \u001b[38;5;28minput\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mWrite a cold sales email\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m result.stream_events():\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m event.type == \u001b[33m\"\u001b[39m\u001b[33mraw_response_event\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event.data, ResponseTextDeltaEvent):\n\u001b[32m      7\u001b[39m         \u001b[38;5;28mprint\u001b[39m(event.data.delta, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\agents\\result.py:358\u001b[39m, in \u001b[36mRunResultStreaming.stream_events\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    355\u001b[39m         \u001b[38;5;28mself\u001b[39m._cleanup_tasks()\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stored_exception:\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stored_exception\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\agents\\run.py:1241\u001b[39m, in \u001b[36mAgentRunner._start_streaming\u001b[39m\u001b[34m(cls, starting_input, streamed_result, starting_agent, max_turns, hooks, context_wrapper, run_config, previous_response_id, auto_previous_response_id, conversation_id, session)\u001b[39m\n\u001b[32m   1230\u001b[39m     streamed_result._input_guardrails_task = asyncio.create_task(\n\u001b[32m   1231\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_input_guardrails_with_queue(\n\u001b[32m   1232\u001b[39m             starting_agent,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1238\u001b[39m         )\n\u001b[32m   1239\u001b[39m     )\n\u001b[32m   1240\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1241\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._run_single_turn_streamed(\n\u001b[32m   1242\u001b[39m         streamed_result,\n\u001b[32m   1243\u001b[39m         current_agent,\n\u001b[32m   1244\u001b[39m         hooks,\n\u001b[32m   1245\u001b[39m         context_wrapper,\n\u001b[32m   1246\u001b[39m         run_config,\n\u001b[32m   1247\u001b[39m         should_run_agent_start_hooks,\n\u001b[32m   1248\u001b[39m         tool_use_tracker,\n\u001b[32m   1249\u001b[39m         all_tools,\n\u001b[32m   1250\u001b[39m         server_conversation_tracker,\n\u001b[32m   1251\u001b[39m     )\n\u001b[32m   1252\u001b[39m     should_run_agent_start_hooks = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1254\u001b[39m     streamed_result.raw_responses = streamed_result.raw_responses + [\n\u001b[32m   1255\u001b[39m         turn_result.model_response\n\u001b[32m   1256\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\agents\\run.py:1514\u001b[39m, in \u001b[36mAgentRunner._run_single_turn_streamed\u001b[39m\u001b[34m(cls, streamed_result, agent, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, all_tools, server_conversation_tracker)\u001b[39m\n\u001b[32m   1509\u001b[39m conversation_id = (\n\u001b[32m   1510\u001b[39m     server_conversation_tracker.conversation_id \u001b[38;5;28;01mif\u001b[39;00m server_conversation_tracker \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1511\u001b[39m )\n\u001b[32m   1513\u001b[39m \u001b[38;5;66;03m# 1. Stream the output events\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1514\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m model.stream_response(\n\u001b[32m   1515\u001b[39m     filtered.instructions,\n\u001b[32m   1516\u001b[39m     filtered.input,\n\u001b[32m   1517\u001b[39m     model_settings,\n\u001b[32m   1518\u001b[39m     all_tools,\n\u001b[32m   1519\u001b[39m     output_schema,\n\u001b[32m   1520\u001b[39m     handoffs,\n\u001b[32m   1521\u001b[39m     get_model_tracing_impl(\n\u001b[32m   1522\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m   1523\u001b[39m     ),\n\u001b[32m   1524\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m   1525\u001b[39m     conversation_id=conversation_id,\n\u001b[32m   1526\u001b[39m     prompt=prompt_config,\n\u001b[32m   1527\u001b[39m ):\n\u001b[32m   1528\u001b[39m     \u001b[38;5;66;03m# Emit the raw event ASAP\u001b[39;00m\n\u001b[32m   1529\u001b[39m     streamed_result._event_queue.put_nowait(RawResponsesStreamEvent(data=event))\n\u001b[32m   1531\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, ResponseCompletedEvent):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\agents\\models\\openai_responses.py:192\u001b[39m, in \u001b[36mOpenAIResponsesModel.stream_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, conversation_id, prompt)\u001b[39m\n\u001b[32m    177\u001b[39m stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m    178\u001b[39m     system_instructions,\n\u001b[32m    179\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    187\u001b[39m     prompt=prompt,\n\u001b[32m    188\u001b[39m )\n\u001b[32m    190\u001b[39m final_response: Response | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunk, ResponseCompletedEvent):\n\u001b[32m    194\u001b[39m         final_response = chunk.response\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\openai\\_streaming.py:148\u001b[39m, in \u001b[36mAsyncStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> AsyncIterator[_T]:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator:\n\u001b[32m    149\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\openai\\_streaming.py:195\u001b[39m, in \u001b[36mAsyncStream.__stream__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    192\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m message \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    193\u001b[39m                     message = \u001b[33m\"\u001b[39m\u001b[33mAn error occurred during streaming\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m APIError(\n\u001b[32m    196\u001b[39m                     message=message,\n\u001b[32m    197\u001b[39m                     request=\u001b[38;5;28mself\u001b[39m.response.request,\n\u001b[32m    198\u001b[39m                     body=data[\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    199\u001b[39m                 )\n\u001b[32m    201\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m process_data(data=data, cast_to=cast_to, response=response)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    204\u001b[39m     \u001b[38;5;66;03m# Ensure the response is closed even if the consumer doesn't read all data\u001b[39;00m\n",
      "\u001b[31mAPIError\u001b[39m: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors."
     ]
    }
   ],
   "source": [
    "# Run an agent\n",
    "\n",
    "result = Runner.run_streamed(sales_agent1, input=\"Write a cold sales email\")\n",
    "\n",
    "async for event in result.stream_events():\n",
    "    if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "        print(event.data.delta, end=\"\", flush=True)\n",
    "        # This prints each chunk immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d190b",
   "metadata": {},
   "source": [
    "Here, we are running the agent in stream mode, which means it will return output token by token instead of after complete output formation.\n",
    "\n",
    "Async loop is loop over items that arrive in the future, asynchronously.\n",
    "- Keep listening to agent output\n",
    "- Each time something new happens, get an event\n",
    "- Process it immediately\n",
    "\n",
    "\n",
    "Agents communicate through events. Each event is an object like:\n",
    "\n",
    "Event(\n",
    "    type=\"raw_response_event\",\n",
    "    data=ResponseTextDeltaEvent(...)\n",
    ")\n",
    "\n",
    "Delta means a small chunk of text, not full response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2d25e4",
   "metadata": {},
   "source": [
    "### Running the agents concurrently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3580d173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting response: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. (request_id: req_ed0b88d34d2f4b57a085c2c266336cf5)\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m message = \u001b[33m\"\u001b[39m\u001b[33mWrite a cold sales email.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trace(\u001b[33m\"\u001b[39m\u001b[33mParallel cold emails\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m      7\u001b[39m         Runner.run(sales_agent1, message),\n\u001b[32m      8\u001b[39m         Runner.run(sales_agent2, message),\n\u001b[32m      9\u001b[39m         Runner.run(sales_agent3, message)\n\u001b[32m     10\u001b[39m     )\n\u001b[32m     12\u001b[39m outputs = [result.final_output \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\agents\\run.py:377\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, auto_previous_response_id, conversation_id, session)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    329\u001b[39m \u001b[33;03mRun a workflow starting at the given agent.\u001b[39;00m\n\u001b[32m    330\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    373\u001b[39m \u001b[33;03m    type of the output.\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    376\u001b[39m runner = DEFAULT_AGENT_RUNNER\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner.run(\n\u001b[32m    378\u001b[39m     starting_agent,\n\u001b[32m    379\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    380\u001b[39m     context=context,\n\u001b[32m    381\u001b[39m     max_turns=max_turns,\n\u001b[32m    382\u001b[39m     hooks=hooks,\n\u001b[32m    383\u001b[39m     run_config=run_config,\n\u001b[32m    384\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    385\u001b[39m     auto_previous_response_id=auto_previous_response_id,\n\u001b[32m    386\u001b[39m     conversation_id=conversation_id,\n\u001b[32m    387\u001b[39m     session=session,\n\u001b[32m    388\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\agents\\run.py:676\u001b[39m, in \u001b[36mAgentRunner.run\u001b[39m\u001b[34m(self, starting_agent, input, **kwargs)\u001b[39m\n\u001b[32m    668\u001b[39m     sequential_results = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    669\u001b[39m         starting_agent,\n\u001b[32m    670\u001b[39m         sequential_guardrails,\n\u001b[32m    671\u001b[39m         _copy_str_or_list(prepared_input),\n\u001b[32m    672\u001b[39m         context_wrapper,\n\u001b[32m    673\u001b[39m     )\n\u001b[32m    675\u001b[39m \u001b[38;5;66;03m# Run parallel guardrails + agent together.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m676\u001b[39m input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    677\u001b[39m     \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    678\u001b[39m         starting_agent,\n\u001b[32m    679\u001b[39m         parallel_guardrails,\n\u001b[32m    680\u001b[39m         _copy_str_or_list(prepared_input),\n\u001b[32m    681\u001b[39m         context_wrapper,\n\u001b[32m    682\u001b[39m     ),\n\u001b[32m    683\u001b[39m     \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    684\u001b[39m         agent=current_agent,\n\u001b[32m    685\u001b[39m         all_tools=all_tools,\n\u001b[32m    686\u001b[39m         original_input=original_input,\n\u001b[32m    687\u001b[39m         generated_items=generated_items,\n\u001b[32m    688\u001b[39m         hooks=hooks,\n\u001b[32m    689\u001b[39m         context_wrapper=context_wrapper,\n\u001b[32m    690\u001b[39m         run_config=run_config,\n\u001b[32m    691\u001b[39m         should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    692\u001b[39m         tool_use_tracker=tool_use_tracker,\n\u001b[32m    693\u001b[39m         server_conversation_tracker=server_conversation_tracker,\n\u001b[32m    694\u001b[39m     ),\n\u001b[32m    695\u001b[39m )\n\u001b[32m    697\u001b[39m \u001b[38;5;66;03m# Combine sequential and parallel results.\u001b[39;00m\n\u001b[32m    698\u001b[39m input_guardrail_results = sequential_results + input_guardrail_results\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\agents\\run.py:1706\u001b[39m, in \u001b[36mAgentRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, server_conversation_tracker)\u001b[39m\n\u001b[32m   1703\u001b[39m     \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m   1704\u001b[39m     \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m-> \u001b[39m\u001b[32m1706\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m   1707\u001b[39m     agent,\n\u001b[32m   1708\u001b[39m     system_prompt,\n\u001b[32m   1709\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1710\u001b[39m     output_schema,\n\u001b[32m   1711\u001b[39m     all_tools,\n\u001b[32m   1712\u001b[39m     handoffs,\n\u001b[32m   1713\u001b[39m     hooks,\n\u001b[32m   1714\u001b[39m     context_wrapper,\n\u001b[32m   1715\u001b[39m     run_config,\n\u001b[32m   1716\u001b[39m     tool_use_tracker,\n\u001b[32m   1717\u001b[39m     server_conversation_tracker,\n\u001b[32m   1718\u001b[39m     prompt_config,\n\u001b[32m   1719\u001b[39m )\n\u001b[32m   1721\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m   1722\u001b[39m     agent=agent,\n\u001b[32m   1723\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1732\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m   1733\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\agents\\run.py:1969\u001b[39m, in \u001b[36mAgentRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, hooks, context_wrapper, run_config, tool_use_tracker, server_conversation_tracker, prompt_config)\u001b[39m\n\u001b[32m   1959\u001b[39m previous_response_id = (\n\u001b[32m   1960\u001b[39m     server_conversation_tracker.previous_response_id\n\u001b[32m   1961\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m server_conversation_tracker\n\u001b[32m   1962\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m server_conversation_tracker.previous_response_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1963\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1964\u001b[39m )\n\u001b[32m   1965\u001b[39m conversation_id = (\n\u001b[32m   1966\u001b[39m     server_conversation_tracker.conversation_id \u001b[38;5;28;01mif\u001b[39;00m server_conversation_tracker \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1967\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1969\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m   1970\u001b[39m     system_instructions=filtered.instructions,\n\u001b[32m   1971\u001b[39m     \u001b[38;5;28minput\u001b[39m=filtered.input,\n\u001b[32m   1972\u001b[39m     model_settings=model_settings,\n\u001b[32m   1973\u001b[39m     tools=all_tools,\n\u001b[32m   1974\u001b[39m     output_schema=output_schema,\n\u001b[32m   1975\u001b[39m     handoffs=handoffs,\n\u001b[32m   1976\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m   1977\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m   1978\u001b[39m     ),\n\u001b[32m   1979\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m   1980\u001b[39m     conversation_id=conversation_id,\n\u001b[32m   1981\u001b[39m     prompt=prompt_config,\n\u001b[32m   1982\u001b[39m )\n\u001b[32m   1984\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m   1986\u001b[39m \u001b[38;5;66;03m# If we have run hooks, or if the agent has hooks, we need to call them after the LLM call\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\agents\\models\\openai_responses.py:97\u001b[39m, in \u001b[36mOpenAIResponsesModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, conversation_id, prompt)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m response_span(disabled=tracing.is_disabled()) \u001b[38;5;28;01mas\u001b[39;00m span_response:\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     98\u001b[39m             system_instructions,\n\u001b[32m     99\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    100\u001b[39m             model_settings,\n\u001b[32m    101\u001b[39m             tools,\n\u001b[32m    102\u001b[39m             output_schema,\n\u001b[32m    103\u001b[39m             handoffs,\n\u001b[32m    104\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    105\u001b[39m             conversation_id=conversation_id,\n\u001b[32m    106\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    107\u001b[39m             prompt=prompt,\n\u001b[32m    108\u001b[39m         )\n\u001b[32m    110\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _debug.DONT_LOG_MODEL_DATA:\n\u001b[32m    111\u001b[39m             logger.debug(\u001b[33m\"\u001b[39m\u001b[33mLLM responded\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\agents\\models\\openai_responses.py:320\u001b[39m, in \u001b[36mOpenAIResponsesModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, previous_response_id, conversation_id, stream, prompt)\u001b[39m\n\u001b[32m    316\u001b[39m         response_format = {\u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: model_settings.verbosity}\n\u001b[32m    318\u001b[39m stream_param: Literal[\u001b[38;5;28;01mTrue\u001b[39;00m] | Omit = \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m omit\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.responses.create(\n\u001b[32m    321\u001b[39m     previous_response_id=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(previous_response_id),\n\u001b[32m    322\u001b[39m     conversation=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(conversation_id),\n\u001b[32m    323\u001b[39m     instructions=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(system_instructions),\n\u001b[32m    324\u001b[39m     model=model_param,\n\u001b[32m    325\u001b[39m     \u001b[38;5;28minput\u001b[39m=list_input,\n\u001b[32m    326\u001b[39m     include=include,\n\u001b[32m    327\u001b[39m     tools=tools_param,\n\u001b[32m    328\u001b[39m     prompt=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(prompt),\n\u001b[32m    329\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.temperature),\n\u001b[32m    330\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.top_p),\n\u001b[32m    331\u001b[39m     truncation=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.truncation),\n\u001b[32m    332\u001b[39m     max_output_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.max_tokens),\n\u001b[32m    333\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    334\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    335\u001b[39m     stream=cast(Any, stream_param),\n\u001b[32m    336\u001b[39m     extra_headers=\u001b[38;5;28mself\u001b[39m._merge_headers(model_settings),\n\u001b[32m    337\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    338\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    339\u001b[39m     text=response_format,\n\u001b[32m    340\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.store),\n\u001b[32m    341\u001b[39m     prompt_cache_retention=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.prompt_cache_retention),\n\u001b[32m    342\u001b[39m     reasoning=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.reasoning),\n\u001b[32m    343\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.metadata),\n\u001b[32m    344\u001b[39m     **extra_args,\n\u001b[32m    345\u001b[39m )\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(Union[Response, AsyncStream[ResponseStreamEvent]], response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\openai\\resources\\responses\\responses.py:2480\u001b[39m, in \u001b[36mAsyncResponses.create\u001b[39m\u001b[34m(self, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, prompt_cache_retention, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2442\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2443\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2444\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2478\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   2479\u001b[39m ) -> Response | AsyncStream[ResponseStreamEvent]:\n\u001b[32m-> \u001b[39m\u001b[32m2480\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2481\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/responses\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2482\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2483\u001b[39m             {\n\u001b[32m   2484\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mbackground\u001b[39m\u001b[33m\"\u001b[39m: background,\n\u001b[32m   2485\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mconversation\u001b[39m\u001b[33m\"\u001b[39m: conversation,\n\u001b[32m   2486\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m: include,\n\u001b[32m   2487\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2488\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minstructions\u001b[39m\u001b[33m\"\u001b[39m: instructions,\n\u001b[32m   2489\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_output_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_output_tokens,\n\u001b[32m   2490\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: max_tool_calls,\n\u001b[32m   2491\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2492\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2493\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2494\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprevious_response_id\u001b[39m\u001b[33m\"\u001b[39m: previous_response_id,\n\u001b[32m   2495\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt,\n\u001b[32m   2496\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2497\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_retention\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_retention,\n\u001b[32m   2498\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m: reasoning,\n\u001b[32m   2499\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2500\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2501\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2502\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2503\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2504\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2505\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: text,\n\u001b[32m   2506\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2507\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2508\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2509\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2510\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtruncation\u001b[39m\u001b[33m\"\u001b[39m: truncation,\n\u001b[32m   2511\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2512\u001b[39m             },\n\u001b[32m   2513\u001b[39m             response_create_params.ResponseCreateParamsStreaming\n\u001b[32m   2514\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2515\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m response_create_params.ResponseCreateParamsNonStreaming,\n\u001b[32m   2516\u001b[39m         ),\n\u001b[32m   2517\u001b[39m         options=make_request_options(\n\u001b[32m   2518\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2519\u001b[39m         ),\n\u001b[32m   2520\u001b[39m         cast_to=Response,\n\u001b[32m   2521\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2522\u001b[39m         stream_cls=AsyncStream[ResponseStreamEvent],\n\u001b[32m   2523\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\openai\\_base_client.py:1797\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1784\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1785\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1792\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1793\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1794\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1795\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1796\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1797\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\openai\\_base_client.py:1597\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1594\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1596\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1597\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1599\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting response: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. (request_id: req_7428fdbdbfa241308acd6c4b0b47988f)\n",
      "Error getting response: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. (request_id: req_5011e2d2451146ad96a68f588a55d846)\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "message = \"Write a cold sales email.\"\n",
    "\n",
    "with trace(\"Parallel cold emails\"):\n",
    "    results = await asyncio.gather(\n",
    "        Runner.run(sales_agent1, message),\n",
    "        Runner.run(sales_agent2, message),\n",
    "        Runner.run(sales_agent3, message)\n",
    "    )\n",
    "\n",
    "outputs = [result.final_output for result in results]\n",
    "\n",
    "for output in outputs:\n",
    "    print(output + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5ff09e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_picker = Agent(\n",
    "    name=\"sales_picker\",\n",
    "    instructions=\"You pick the best cold sales email from the given options. \\\n",
    "Imagine you are a customer and pick the one you are most likely to respond to. \\\n",
    "Do not give an explanation; reply with the selected email only.\",\n",
    "    model=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9cf52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting response: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. (request_id: req_4a10967fff47471fa574b586dc55adb1)\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m message = \u001b[33m\"\u001b[39m\u001b[33mWrite a cold sales email\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trace(\u001b[33m\"\u001b[39m\u001b[33mSelection from sales people\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m      5\u001b[39m         Runner.run(sales_agent1, message),\n\u001b[32m      6\u001b[39m         Runner.run(sales_agent2, message),\n\u001b[32m      7\u001b[39m         Runner.run(sales_agent3, message),\n\u001b[32m      8\u001b[39m     )\n\u001b[32m      9\u001b[39m     outputs = [result.final_output \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[32m     11\u001b[39m     emails = \u001b[33m\"\u001b[39m\u001b[33mCold sales emails:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEmail:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\agents\\run.py:377\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, auto_previous_response_id, conversation_id, session)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    329\u001b[39m \u001b[33;03mRun a workflow starting at the given agent.\u001b[39;00m\n\u001b[32m    330\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    373\u001b[39m \u001b[33;03m    type of the output.\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    376\u001b[39m runner = DEFAULT_AGENT_RUNNER\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner.run(\n\u001b[32m    378\u001b[39m     starting_agent,\n\u001b[32m    379\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    380\u001b[39m     context=context,\n\u001b[32m    381\u001b[39m     max_turns=max_turns,\n\u001b[32m    382\u001b[39m     hooks=hooks,\n\u001b[32m    383\u001b[39m     run_config=run_config,\n\u001b[32m    384\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    385\u001b[39m     auto_previous_response_id=auto_previous_response_id,\n\u001b[32m    386\u001b[39m     conversation_id=conversation_id,\n\u001b[32m    387\u001b[39m     session=session,\n\u001b[32m    388\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\agents\\run.py:676\u001b[39m, in \u001b[36mAgentRunner.run\u001b[39m\u001b[34m(self, starting_agent, input, **kwargs)\u001b[39m\n\u001b[32m    668\u001b[39m     sequential_results = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    669\u001b[39m         starting_agent,\n\u001b[32m    670\u001b[39m         sequential_guardrails,\n\u001b[32m    671\u001b[39m         _copy_str_or_list(prepared_input),\n\u001b[32m    672\u001b[39m         context_wrapper,\n\u001b[32m    673\u001b[39m     )\n\u001b[32m    675\u001b[39m \u001b[38;5;66;03m# Run parallel guardrails + agent together.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m676\u001b[39m input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    677\u001b[39m     \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    678\u001b[39m         starting_agent,\n\u001b[32m    679\u001b[39m         parallel_guardrails,\n\u001b[32m    680\u001b[39m         _copy_str_or_list(prepared_input),\n\u001b[32m    681\u001b[39m         context_wrapper,\n\u001b[32m    682\u001b[39m     ),\n\u001b[32m    683\u001b[39m     \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    684\u001b[39m         agent=current_agent,\n\u001b[32m    685\u001b[39m         all_tools=all_tools,\n\u001b[32m    686\u001b[39m         original_input=original_input,\n\u001b[32m    687\u001b[39m         generated_items=generated_items,\n\u001b[32m    688\u001b[39m         hooks=hooks,\n\u001b[32m    689\u001b[39m         context_wrapper=context_wrapper,\n\u001b[32m    690\u001b[39m         run_config=run_config,\n\u001b[32m    691\u001b[39m         should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    692\u001b[39m         tool_use_tracker=tool_use_tracker,\n\u001b[32m    693\u001b[39m         server_conversation_tracker=server_conversation_tracker,\n\u001b[32m    694\u001b[39m     ),\n\u001b[32m    695\u001b[39m )\n\u001b[32m    697\u001b[39m \u001b[38;5;66;03m# Combine sequential and parallel results.\u001b[39;00m\n\u001b[32m    698\u001b[39m input_guardrail_results = sequential_results + input_guardrail_results\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\agents\\run.py:1706\u001b[39m, in \u001b[36mAgentRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, server_conversation_tracker)\u001b[39m\n\u001b[32m   1703\u001b[39m     \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m   1704\u001b[39m     \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m-> \u001b[39m\u001b[32m1706\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m   1707\u001b[39m     agent,\n\u001b[32m   1708\u001b[39m     system_prompt,\n\u001b[32m   1709\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1710\u001b[39m     output_schema,\n\u001b[32m   1711\u001b[39m     all_tools,\n\u001b[32m   1712\u001b[39m     handoffs,\n\u001b[32m   1713\u001b[39m     hooks,\n\u001b[32m   1714\u001b[39m     context_wrapper,\n\u001b[32m   1715\u001b[39m     run_config,\n\u001b[32m   1716\u001b[39m     tool_use_tracker,\n\u001b[32m   1717\u001b[39m     server_conversation_tracker,\n\u001b[32m   1718\u001b[39m     prompt_config,\n\u001b[32m   1719\u001b[39m )\n\u001b[32m   1721\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m   1722\u001b[39m     agent=agent,\n\u001b[32m   1723\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1732\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m   1733\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\agents\\run.py:1969\u001b[39m, in \u001b[36mAgentRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, hooks, context_wrapper, run_config, tool_use_tracker, server_conversation_tracker, prompt_config)\u001b[39m\n\u001b[32m   1959\u001b[39m previous_response_id = (\n\u001b[32m   1960\u001b[39m     server_conversation_tracker.previous_response_id\n\u001b[32m   1961\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m server_conversation_tracker\n\u001b[32m   1962\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m server_conversation_tracker.previous_response_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1963\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1964\u001b[39m )\n\u001b[32m   1965\u001b[39m conversation_id = (\n\u001b[32m   1966\u001b[39m     server_conversation_tracker.conversation_id \u001b[38;5;28;01mif\u001b[39;00m server_conversation_tracker \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1967\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1969\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m   1970\u001b[39m     system_instructions=filtered.instructions,\n\u001b[32m   1971\u001b[39m     \u001b[38;5;28minput\u001b[39m=filtered.input,\n\u001b[32m   1972\u001b[39m     model_settings=model_settings,\n\u001b[32m   1973\u001b[39m     tools=all_tools,\n\u001b[32m   1974\u001b[39m     output_schema=output_schema,\n\u001b[32m   1975\u001b[39m     handoffs=handoffs,\n\u001b[32m   1976\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m   1977\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m   1978\u001b[39m     ),\n\u001b[32m   1979\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m   1980\u001b[39m     conversation_id=conversation_id,\n\u001b[32m   1981\u001b[39m     prompt=prompt_config,\n\u001b[32m   1982\u001b[39m )\n\u001b[32m   1984\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m   1986\u001b[39m \u001b[38;5;66;03m# If we have run hooks, or if the agent has hooks, we need to call them after the LLM call\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\agents\\models\\openai_responses.py:97\u001b[39m, in \u001b[36mOpenAIResponsesModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, conversation_id, prompt)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m response_span(disabled=tracing.is_disabled()) \u001b[38;5;28;01mas\u001b[39;00m span_response:\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     98\u001b[39m             system_instructions,\n\u001b[32m     99\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    100\u001b[39m             model_settings,\n\u001b[32m    101\u001b[39m             tools,\n\u001b[32m    102\u001b[39m             output_schema,\n\u001b[32m    103\u001b[39m             handoffs,\n\u001b[32m    104\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    105\u001b[39m             conversation_id=conversation_id,\n\u001b[32m    106\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    107\u001b[39m             prompt=prompt,\n\u001b[32m    108\u001b[39m         )\n\u001b[32m    110\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _debug.DONT_LOG_MODEL_DATA:\n\u001b[32m    111\u001b[39m             logger.debug(\u001b[33m\"\u001b[39m\u001b[33mLLM responded\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\agents\\models\\openai_responses.py:320\u001b[39m, in \u001b[36mOpenAIResponsesModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, previous_response_id, conversation_id, stream, prompt)\u001b[39m\n\u001b[32m    316\u001b[39m         response_format = {\u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: model_settings.verbosity}\n\u001b[32m    318\u001b[39m stream_param: Literal[\u001b[38;5;28;01mTrue\u001b[39;00m] | Omit = \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m omit\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.responses.create(\n\u001b[32m    321\u001b[39m     previous_response_id=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(previous_response_id),\n\u001b[32m    322\u001b[39m     conversation=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(conversation_id),\n\u001b[32m    323\u001b[39m     instructions=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(system_instructions),\n\u001b[32m    324\u001b[39m     model=model_param,\n\u001b[32m    325\u001b[39m     \u001b[38;5;28minput\u001b[39m=list_input,\n\u001b[32m    326\u001b[39m     include=include,\n\u001b[32m    327\u001b[39m     tools=tools_param,\n\u001b[32m    328\u001b[39m     prompt=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(prompt),\n\u001b[32m    329\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.temperature),\n\u001b[32m    330\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.top_p),\n\u001b[32m    331\u001b[39m     truncation=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.truncation),\n\u001b[32m    332\u001b[39m     max_output_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.max_tokens),\n\u001b[32m    333\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    334\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    335\u001b[39m     stream=cast(Any, stream_param),\n\u001b[32m    336\u001b[39m     extra_headers=\u001b[38;5;28mself\u001b[39m._merge_headers(model_settings),\n\u001b[32m    337\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    338\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    339\u001b[39m     text=response_format,\n\u001b[32m    340\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.store),\n\u001b[32m    341\u001b[39m     prompt_cache_retention=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.prompt_cache_retention),\n\u001b[32m    342\u001b[39m     reasoning=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.reasoning),\n\u001b[32m    343\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.metadata),\n\u001b[32m    344\u001b[39m     **extra_args,\n\u001b[32m    345\u001b[39m )\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(Union[Response, AsyncStream[ResponseStreamEvent]], response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\openai\\resources\\responses\\responses.py:2480\u001b[39m, in \u001b[36mAsyncResponses.create\u001b[39m\u001b[34m(self, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, prompt_cache_retention, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2442\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2443\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2444\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2478\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   2479\u001b[39m ) -> Response | AsyncStream[ResponseStreamEvent]:\n\u001b[32m-> \u001b[39m\u001b[32m2480\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2481\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/responses\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2482\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2483\u001b[39m             {\n\u001b[32m   2484\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mbackground\u001b[39m\u001b[33m\"\u001b[39m: background,\n\u001b[32m   2485\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mconversation\u001b[39m\u001b[33m\"\u001b[39m: conversation,\n\u001b[32m   2486\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m: include,\n\u001b[32m   2487\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2488\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minstructions\u001b[39m\u001b[33m\"\u001b[39m: instructions,\n\u001b[32m   2489\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_output_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_output_tokens,\n\u001b[32m   2490\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: max_tool_calls,\n\u001b[32m   2491\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2492\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2493\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2494\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprevious_response_id\u001b[39m\u001b[33m\"\u001b[39m: previous_response_id,\n\u001b[32m   2495\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt,\n\u001b[32m   2496\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2497\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_retention\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_retention,\n\u001b[32m   2498\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m: reasoning,\n\u001b[32m   2499\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2500\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2501\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2502\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2503\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2504\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2505\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: text,\n\u001b[32m   2506\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2507\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2508\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2509\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2510\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtruncation\u001b[39m\u001b[33m\"\u001b[39m: truncation,\n\u001b[32m   2511\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2512\u001b[39m             },\n\u001b[32m   2513\u001b[39m             response_create_params.ResponseCreateParamsStreaming\n\u001b[32m   2514\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2515\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m response_create_params.ResponseCreateParamsNonStreaming,\n\u001b[32m   2516\u001b[39m         ),\n\u001b[32m   2517\u001b[39m         options=make_request_options(\n\u001b[32m   2518\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2519\u001b[39m         ),\n\u001b[32m   2520\u001b[39m         cast_to=Response,\n\u001b[32m   2521\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2522\u001b[39m         stream_cls=AsyncStream[ResponseStreamEvent],\n\u001b[32m   2523\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\openai\\_base_client.py:1797\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1784\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1785\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1792\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1793\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1794\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1795\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1796\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1797\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Agentic_AI\\AI_Agents\\AI-Agents\\venv\\Lib\\site-packages\\openai\\_base_client.py:1597\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1594\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1596\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1597\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1599\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting response: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. (request_id: req_4330aecac8ab4307893fe145fc0fe83b)\n",
      "Error getting response: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. (request_id: req_b96f158ece924ecaa2612886ba54c65e)\n"
     ]
    }
   ],
   "source": [
    "message = \"Write a cold sales email\"\n",
    "\n",
    "with trace(\"Selection from sales people\"):\n",
    "    results = await asyncio.gather(\n",
    "        Runner.run(sales_agent1, message),\n",
    "        Runner.run(sales_agent2, message),\n",
    "        Runner.run(sales_agent3, message),\n",
    "    )\n",
    "    outputs = [result.final_output for result in results]\n",
    "\n",
    "    emails = \"Cold sales emails:\\n\\n\" + \"\\n\\nEmail:\\n\\n\".join(outputs)\n",
    "\n",
    "    best = await Runner.run(sales_picker, emails)\n",
    "\n",
    "    print(f\"Best sales email:\\n{best.final_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce07e76a",
   "metadata": {},
   "source": [
    "## Use of Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2fd186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_agent1 = Agent(\n",
    "        name=\"Professional Sales Agent\",\n",
    "        instructions=instructions1,\n",
    "        model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "sales_agent2 = Agent(\n",
    "        name=\"Engaging Sales Agent\",\n",
    "        instructions=instructions2,\n",
    "        model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "sales_agent3 = Agent(\n",
    "        name=\"Busy Sales Agent\",\n",
    "        instructions=instructions3,\n",
    "        model=\"gpt-4o-mini\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f697b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def send_email(subject: str, html_body: str) -> Dict[str, str]:\n",
    "    \"\"\" Send out an email with the given body to all sales prospects \"\"\"\n",
    "    configuration = brevo_python.Configuration()\n",
    "    configuration.api_key[\"api-key\"] = os.getenv(\"BREVO_API_KEY\")\n",
    "    api_client = brevo_python.ApiClient(configuration)\n",
    "    email_api = TransactionalEmailsApi(api_client)\n",
    "    email = SendSmtpEmail(\n",
    "        sender={\"name\": \"Ayushi Saxena\", \"email\": \"khonthedark@gmail.com\"},\n",
    "        to=[{\"email\": \"windwatcher17@gmail.com\", \"name\": \"Receiver\"}],\n",
    "        subject = subject,\n",
    "        html_content = html_body\n",
    "    )\n",
    "    email_api.send_transac_email(email)\n",
    "    return {\"status\": \"success\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b569412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionTool(name='send_email', description='Send out an email with the given body to all sales prospects', params_json_schema={'properties': {'body': {'title': 'Body', 'type': 'string'}}, 'required': ['body'], 'title': 'send_email_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x0000015D23CC1BC0>, strict_json_schema=True, is_enabled=True, tool_input_guardrails=None, tool_output_guardrails=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at it\n",
    "send_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1dd5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool1 = sales_agent1.as_tool(tool_name=\"sales_agent1\", tool_description=\"Write a cold sales email\")\n",
    "tool1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abacafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"Write a cold sales email\"\n",
    "\n",
    "tool1 = sales_agent1.as_tool(tool_name=\"sales_agent1\", tool_description=description)\n",
    "tool2 = sales_agent2.as_tool(tool_name=\"sales_agent2\", tool_description=description)\n",
    "tool3 = sales_agent3.as_tool(tool_name=\"sales_agent3\", tool_description=description)\n",
    "\n",
    "tools = [tool1, tool2, tool3, send_email]\n",
    "\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b64b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You are a Sales Manager at ComplAI. Your goal is to find the single best cold sales email using the sales_agent tools.\n",
    " \n",
    "Follow these steps carefully:\n",
    "1. Generate Drafts: Use all three sales_agent tools to generate three different email drafts. Do not proceed until all three drafts are ready.\n",
    " \n",
    "2. Evaluate and Select: Review the drafts and choose the single best email using your judgment of which one is most effective.\n",
    " \n",
    "3. Use the send_email tool to send the best email (and only the best email) to the user.\n",
    " \n",
    "Crucial Rules:\n",
    "- You must use the sales agent tools to generate the drafts â€” do not write them yourself.\n",
    "- You must send ONE email using the send_email tool â€” never more than one.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "sales_manager = Agent(name=\"Sales Manager\", instructions=instructions, tools=tools, model=\"gpt-4o-mini\")\n",
    "\n",
    "message = \"Send a cold sales email addressed to 'Dear CEO'\"\n",
    "\n",
    "with trace(\"Sales manager\"):\n",
    "    result = await Runner.run(sales_manager, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subject_instructions = \"You can write a subject for a cold sales email. \\\n",
    "You are given a message and you need to write a subject for an email that is likely to get a response.\"\n",
    "\n",
    "html_instructions = \"You can convert a text email body to an HTML email body. \\\n",
    "You are given a text email body which might have some markdown \\\n",
    "and you need to convert it to an HTML email body with simple, clear, compelling layout and design.\"\n",
    "\n",
    "subject_writer = Agent(name=\"Email subject writer\", instructions=subject_instructions, model=\"gpt-4o-mini\")\n",
    "subject_tool = subject_writer.as_tool(tool_name=\"subject_writer\", tool_description=\"Write a subject for a cold sales email\")\n",
    "\n",
    "html_converter = Agent(name=\"HTML email body converter\", instructions=html_instructions, model=\"gpt-4o-mini\")\n",
    "html_tool = html_converter.as_tool(tool_name=\"html_converter\",tool_description=\"Convert a text email body to an HTML email body\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3172e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [subject_tool, html_tool, send_email]\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c0540",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions =\"You are an email formatter and sender. You receive the body of an email to be sent. \\\n",
    "You first use the subject_writer tool to write a subject for the email, then use the html_converter tool to convert the body to HTML. \\\n",
    "Finally, you use the send_html_email tool to send the email with the subject and HTML body.\"\n",
    "\n",
    "\n",
    "emailer_agent = Agent(\n",
    "    name=\"Email Manager\",\n",
    "    instructions=instructions,\n",
    "    tools=tools,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    handoff_description=\"Convert an email to HTML and send it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b70d800",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [tool1, tool2, tool3]\n",
    "handoffs = [emailer_agent]\n",
    "print(tools)\n",
    "print(handoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d707dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_manager_instructions = \"\"\"\n",
    "You are a Sales Manager at ComplAI. Your goal is to find the single best cold sales email using the sales_agent tools.\n",
    " \n",
    "Follow these steps carefully:\n",
    "1. Generate Drafts: Use all three sales_agent tools to generate three different email drafts. Do not proceed until all three drafts are ready.\n",
    " \n",
    "2. Evaluate and Select: Review the drafts and choose the single best email using your judgment of which one is most effective.\n",
    "You can use the tools multiple times if you're not satisfied with the results from the first try.\n",
    " \n",
    "3. Handoff for Sending: Pass ONLY the winning email draft to the 'Email Manager' agent. The Email Manager will take care of formatting and sending.\n",
    " \n",
    "Crucial Rules:\n",
    "- You must use the sales agent tools to generate the drafts â€” do not write them yourself.\n",
    "- You must hand off exactly ONE email to the Email Manager â€” never more than one.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "sales_manager = Agent(\n",
    "    name=\"Sales Manager\",\n",
    "    instructions=sales_manager_instructions,\n",
    "    tools=tools,\n",
    "    handoffs=handoffs,\n",
    "    model=\"gpt-4o-mini\")\n",
    "\n",
    "message = \"Send out a cold sales email addressed to Dear CEO from Alice\"\n",
    "\n",
    "with trace(\"Automated SDR\"):\n",
    "    result = await Runner.run(sales_manager, message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
